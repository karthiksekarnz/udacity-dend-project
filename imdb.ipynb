{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id, collect_list\n",
    "from pyspark.sql.types import TimestampType, IntegerType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('settings.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config.get('KEYS', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config.get('KEYS', 'AWS_SECRET_ACCESS_KEY')\n",
    "input_dir = config.get('FILES', 'INPUT_DIR')\n",
    "output_dir = config.get('FILES', 'OUTPUT_DIR')\n",
    "current_year = datetime.now().year\n",
    "filter_by_region = config.get('FILTERS', 'region')\n",
    "\n",
    "imdb_input = input_dir + \"/imdb\"\n",
    "tmdb_input = input_dir + \"/tmdb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Basic title\n",
    "imdb_titles_df = spark.read.options(header=True, inferSchema=True, nullValue=\"\\\\N\") \\\n",
    "        .csv(imdb_input + '/title.basics.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_movie_titles_df = imdb_titles_df.select(\n",
    "    col(\"tconst\").alias(\"title_id\"),\n",
    "    col(\"titleType\").alias(\"title_type\"),\n",
    "    col(\"primaryTitle\").alias(\"primary_title\"),\n",
    "    col(\"originalTitle\").alias(\"original_title\"),\n",
    "    col(\"genres\"),\n",
    "    col(\"runtimeMinutes\").alias(\"runtime\"),\n",
    "    col(\"isAdult\").alias(\"is_adult\"),\n",
    "    col(\"startYear\").alias(\"start_year\"),\n",
    "    col(\"endYear\").alias(\"end_year\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_movie_titles_df = imdb_movie_titles_df.filter((imdb_titles_df.titleType == 'movie') & (imdb_titles_df['isAdult'] == '0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_title_akas_df = spark.read.options(header=True, inferSchema=True, nullValue=\"\\\\N\") \\\n",
    "    .csv(imdb_input + '/title.akas.tsv.gz', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_titles = imdb_movie_titles_df \\\n",
    "    .join(imdb_title_akas_df, imdb_movie_titles_df.title_id == imdb_title_akas_df.titleId, \"left\") \\\n",
    "    .select(col(\"titleId\").alias(\"imdb_title_id\"),\n",
    "            imdb_title_akas_df.title,\n",
    "            \"language\",\n",
    "            \"region\",\n",
    "            \"is_adult\",\n",
    "            \"start_year\",\n",
    "            ).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_titles = movies_titles.filter((col(\"language\") == \"en\") & col(\"region\").isNotNull() & col(\"start_year\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imdb_title_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- ordering: integer (nullable = true)\n",
      " |-- is_original_title: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- is_adult: integer (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_by_region = config.get('FILTERS', 'region')\n",
    "filter_by_year = config.get('FILTERS', 'year')\n",
    "\n",
    "condition = None\n",
    "\n",
    "if filter_by_year and filter_by_region:\n",
    "    condition = \"start_year == {0} and region == {1}\".format(filter_by_year, filter_by_region)\n",
    "\n",
    "elif filter_by_region and not filter_by_year:\n",
    "    condition = \"region == {0}\".format(filter_by_region)\n",
    "\n",
    "elif filter_by_year and not filter_by_region:\n",
    "    condition = \"start_year == {0}\".format(filter_by_year)\n",
    "\n",
    "if condition is not None:\n",
    "    movies_titles = movies_titles.filter(condition)\n",
    "    \n",
    "movies_titles.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_titles.write.partitionBy(\"region\", \"start_year\").format(\"parquet\") \\\n",
    "        .save(output_dir + '/movies_titles', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TMDB movies list\n",
    "tmdb_movies_df = spark.read.options(inferSchema=True, nullValue=\"\\\\N\").json(tmdb_input+'/movies_list', multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmdb_movies = tmdb_movies_df.select(\"id\", \"imdb_id\", \"revenue\", \"budget\", \"overview\", \"popularity\", \"release_date\", \"status\", \"vote_average\", \"vote_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmdb_movies_df= spark.read.parquet(output_dir + '/tmdb_movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- imdb_id: string (nullable = true)\n",
      " |-- revenue: long (nullable = true)\n",
      " |-- budget: long (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- popularity: double (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- vote_average: double (nullable = true)\n",
      " |-- vote_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmdb_movies.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_titles_df = spark.read.parquet(output_dir + '/movies_titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imdb_title_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- is_original_title: integer (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- is_adult: integer (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_titles_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imdb_title_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- original_title: string (nullable = true)\n",
      " |-- primary_title: string (nullable = true)\n",
      " |-- is_original_title: integer (nullable = true)\n",
      " |-- ordering: integer (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- overview: string (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      " |-- runtime: string (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Movie details\n",
    "\n",
    "imdb_movie_titles_df = imdb_titles_df.select(\n",
    "    col(\"tconst\").alias(\"imdb_title_id\"),\n",
    "    col(\"titleType\").alias(\"title_type\"),\n",
    "    col(\"primaryTitle\").alias(\"primary_title\"),\n",
    "    col(\"originalTitle\").alias(\"original_title\"),\n",
    "    col(\"genres\"),\n",
    "    col(\"runtimeMinutes\").alias(\"runtime\"),\n",
    "    col(\"isAdult\").alias(\"is_adult\"),\n",
    "    col(\"startYear\").alias(\"start_year\"),\n",
    ")\n",
    "\n",
    "imdb_movie_titles_df = imdb_movie_titles_df.filter((imdb_movie_titles_df['title_type'] == 'movie') & (imdb_movie_titles_df['is_adult'] == '0'))\n",
    "\n",
    "# movies_details query combining imdb & tmdb movies\n",
    "movies_details = imdb_movie_titles_df \\\n",
    "    .join(imdb_title_akas_df, imdb_movie_titles_df.imdb_title_id == imdb_title_akas_df.titleId, \"left\")\\\n",
    "    .join(tmdb_movies_df, imdb_movie_titles_df.imdb_title_id == tmdb_movies_df.imdb_id, \"left\") \\\n",
    "    .select(\n",
    "        \"imdb_title_id\",\n",
    "        imdb_title_akas_df.title,\n",
    "        imdb_movie_titles_df.original_title,\n",
    "        imdb_movie_titles_df.primary_title,\n",
    "        col(\"isOriginalTitle\").alias(\"is_original_title\"),\n",
    "        imdb_title_akas_df.ordering,\n",
    "        imdb_movie_titles_df.genres,\n",
    "        imdb_title_akas_df.language,\n",
    "        imdb_title_akas_df.region,\n",
    "        tmdb_movies_df.overview,\n",
    "        imdb_movie_titles_df.start_year,\n",
    "        imdb_movie_titles_df.runtime,\n",
    "        tmdb_movies_df.release_date\n",
    ")\n",
    "\n",
    "filter_by_region = config.get('FILTERS', 'region')\n",
    "filter_by_year = config.get('FILTERS', 'year')\n",
    "\n",
    "condition = None\n",
    "\n",
    "if filter_by_year and filter_by_region:\n",
    "    condition = \"start_year == {0} and region == {1}\".format(filter_by_year, filter_by_region)\n",
    "\n",
    "elif filter_by_region and not filter_by_year:\n",
    "    condition = \"region == {0}\".format(filter_by_region)\n",
    "\n",
    "elif filter_by_year and not filter_by_region:\n",
    "    condition = \"start_year == {0}\".format(filter_by_year)\n",
    "\n",
    "if condition is not None:\n",
    "    movies_details = movies_details.filter(condition)\n",
    "    \n",
    "movies_details.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9b8cffa90266>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# movies_details table parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmovies_details\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"region\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"start_year\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/movies_details'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# movies_details table parquet\n",
    "movies_details.write.partitionBy(\"region\", \"start_year\").format(\"parquet\").save(output_dir + '/movies_titles_details', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- averageRating: double (nullable = true)\n",
      " |-- numVotes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#IMDB ratings DF\n",
    "imdb_ratings_df = spark.read.options(header=True, inferSchema=True, nullValue=\"\\\\N\").csv(imdb_input + '/title.ratings.tsv.gz', sep='\\t')\n",
    "imdb_ratings_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imdb_title_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      " |-- imdb_total_votes: integer (nullable = true)\n",
      " |-- imdb_avg_rating: double (nullable = true)\n",
      " |-- tmdb_total_votes: long (nullable = true)\n",
      " |-- tmdb_avg_rating: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_ratings = movies_titles_df.join(imdb_ratings_df, movies_titles_df.imdb_title_id == imdb_ratings_df.tconst, \"left\") \\\n",
    "        .join(tmdb_movies_df, movies_titles_df.imdb_title_id == tmdb_movies_df.imdb_id, \"left\") \\\n",
    "        .select(\n",
    "        \"imdb_title_id\",\n",
    "        movies_titles_df.title,\n",
    "        \"region\",\n",
    "        \"language\",\n",
    "        \"start_year\",\n",
    "        col(\"numVotes\").alias(\"imdb_total_votes\"),\n",
    "        col(\"averageRating\").alias(\"imdb_avg_rating\"),\n",
    "        col(\"vote_count\").alias(\"tmdb_total_votes\"),\n",
    "        col(\"vote_average\").alias(\"tmdb_avg_rating\")\n",
    ")\n",
    "\n",
    "movies_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#IMDB Ratings table write parquet\n",
    "movies_ratings.write.partitionBy(\"region\", \"start_year\").format(\"parquet\").save(output_dir + '/movies_ratings', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- primaryName: string (nullable = true)\n",
      " |-- birthYear: string (nullable = true)\n",
      " |-- deathYear: string (nullable = true)\n",
      " |-- primaryProfession: string (nullable = true)\n",
      " |-- knownForTitles: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_names_df = spark.read.option(\"header\", \"true\").csv(imdb_input + '/name.basics.tsv.gz', sep='\\t')\n",
    "imdb_names_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "moviesTitlesMap = movies_details.select(\"imdb_title_id\").distinct().rdd.map(lambda row: row['imdb_title_id']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "distinct_titleIds = movies_details.select(\"imdb_title_id\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tconst: string (nullable = true)\n",
      " |-- ordering: integer (nullable = true)\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- characters: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_principals_df = spark.read.options(header=True, inferSchema=True, nullValue=\"\\\\N\").csv(imdb_input + '/title.principals.tsv.gz', sep='\\t')\n",
    "imdb_principals_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_principal_crew = imdb_principals_df.select(\n",
    "    col(\"tconst\").alias(\"imdb_title_id\"),\n",
    "    \"ordering\",\n",
    "    col(\"nconst\").alias(\"imdb_name_id\"),\n",
    "    \"category\",\n",
    "    \"job\",\n",
    "    \"characters\"\n",
    ").filter(imdb_principals_df['tconst'].isin(sampleIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_principal_crew.write.partitionBy(\"imdb_title_id\").format(\"parquet\").save(output_dir + '/movies_principal_crew', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movieCrewNameIds = imdb_principal_crew.select(\"imdb_name_id\").distinct().rdd.map(lambda row: row['imdb_name_id']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- nconst: string (nullable = true)\n",
      " |-- primaryName: string (nullable = true)\n",
      " |-- birthYear: integer (nullable = true)\n",
      " |-- deathYear: integer (nullable = true)\n",
      " |-- primaryProfession: string (nullable = true)\n",
      " |-- knownForTitles: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_names_df = spark.read.options(header=True, inferSchema=True, nullValue=\"\\\\N\").csv(imdb_input + '/name.basics.tsv.gz', sep='\\t')\n",
    "imdb_names_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "imdb_crew_names = imdb_names_df.select(\n",
    "    col(\"nconst\").alias(\"imdb_name_id\"),\n",
    "    col(\"primaryName\").alias(\"primary_name\"),\n",
    "    col(\"birthYear\").alias(\"birth_year\"),\n",
    "    col(\"deathYear\").alias(\"death_year\"),\n",
    "    col(\"primaryProfession\").alias(\"primary_profession\"),\n",
    "    col(\"knownForTitles\").alias(\"known_for_titles\")\n",
    ").filter(imdb_names_df['nconst'].isin(movieCrewNameIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imdb_title_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      " |-- revenue: long (nullable = true)\n",
      " |-- budget: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Movie finance\n",
    "movies_finances = movies_titles_df\\\n",
    "        .join(tmdb_movies_df, movies_titles_df.imdb_title_id == tmdb_movies_df.imdb_id)\\\n",
    "        .select(\n",
    "            col(\"imdb_title_id\"),\n",
    "            movies_titles_df.title,\n",
    "            movies_titles_df.language,\n",
    "            movies_titles_df.region,\n",
    "            movies_titles_df.start_year,\n",
    "            col(\"revenue\"),\n",
    "            col(\"budget\")\n",
    "        )\n",
    "\n",
    "movies_finances.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_finances.write.partitionBy(\"region\", \"start_year\").format(\"parquet\").save(output_dir + '/movies_finances', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_ratings = spark.read.parquet(output_dir + \"/movies_ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imdb_title_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- imdb_total_votes: integer (nullable = true)\n",
      " |-- imdb_avg_rating: double (nullable = true)\n",
      " |-- tmdb_total_votes: long (nullable = true)\n",
      " |-- tmdb_avg_rating: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- start_year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies_ratings.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_ratings.createOrReplaceTempView(\"movies_ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "top_american_english_movies = spark.sql('''SELECT \n",
    "              imdb_title_id, \n",
    "              collect_list(title) as title, \n",
    "              imdb_total_votes, \n",
    "              imdb_avg_rating, \n",
    "              region, \n",
    "              language, \n",
    "              start_year \n",
    "            FROM movies_ratings \n",
    "            WHERE language IS NULL OR language = 'en' \n",
    "            GROUP BY \n",
    "              imdb_title_id, \n",
    "              imdb_total_votes, \n",
    "              imdb_avg_rating, \n",
    "              region, \n",
    "              language, \n",
    "              start_year \n",
    "            ORDER BY imdb_total_votes DESC''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------------------------------------------+----------------+---------------+------+--------+----------+\n",
      "|imdb_title_id|title                                                 |imdb_total_votes|imdb_avg_rating|region|language|start_year|\n",
      "+-------------+------------------------------------------------------+----------------+---------------+------+--------+----------+\n",
      "|tt10872600   |[Spider-Man: No Way Home, Serenity Now]               |521999          |8.6            |US    |null    |2021      |\n",
      "|tt1160419    |[Dune]                                                |512910          |8.1            |US    |null    |2021      |\n",
      "|tt11286314   |[Don't Look Up]                                       |462155          |7.2            |US    |null    |2021      |\n",
      "|tt12361974   |[The Snyder Cut, Zack Snyder's Justice League]        |364815          |8.1            |US    |null    |2021      |\n",
      "|tt3480822    |[Blue Bayou, Black Widow]                             |341616          |6.7            |US    |null    |2021      |\n",
      "|tt9376612    |[Steamboat, Shang-Chi and the Legend of the Ten Rings]|334902          |7.5            |US    |null    |2021      |\n",
      "|tt2382320    |[No Time to Die]                                      |327480          |7.3            |US    |null    |2021      |\n",
      "|tt6334354    |[The Suicide Squad]                                   |313874          |7.2            |US    |null    |2021      |\n",
      "|tt6264654    |[Free Guy]                                            |308222          |7.2            |US    |null    |2021      |\n",
      "|tt9032400    |[Eternals]                                            |274107          |6.4            |US    |null    |2021      |\n",
      "+-------------+------------------------------------------------------+----------------+---------------+------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_american_english_movies.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "movies_finances = spark.read.parquet(output_dir + '/movies_finances')\n",
    "movies_finances.createOrReplaceTempView(\"movies_finances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`imdb_title_id`' given input columns: [movies_finances.title, movies_finances.language, movies_finances.start_year, movies_finances.region, movies_finances.revenue, movies_finances.imdb_id, movies_finances.budget]; line 14 pos 6;\\n'Sort ['revenue DESC NULLS LAST], true\\n+- 'Aggregate ['imdb_title_id, revenue#803L, budget#804L, region#805, language#802, start_year#806], ['imdb_title_id, collect_list(title#801, 0, 0) AS title#814, revenue#803L, budget#804L, region#805, language#802, start_year#806]\\n   +- Filter (isnotnull(revenue#803L) && (isnull(language#802) || (language#802 = en)))\\n      +- SubqueryAlias `movies_finances`\\n         +- Relation[imdb_id#800,title#801,language#802,revenue#803L,budget#804L,region#805,start_year#806] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.sql.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`imdb_title_id`' given input columns: [movies_finances.title, movies_finances.language, movies_finances.start_year, movies_finances.region, movies_finances.revenue, movies_finances.imdb_id, movies_finances.budget]; line 14 pos 6;\n'Sort ['revenue DESC NULLS LAST], true\n+- 'Aggregate ['imdb_title_id, revenue#803L, budget#804L, region#805, language#802, start_year#806], ['imdb_title_id, collect_list(title#801, 0, 0) AS title#814, revenue#803L, budget#804L, region#805, language#802, start_year#806]\n   +- Filter (isnotnull(revenue#803L) && (isnull(language#802) || (language#802 = en)))\n      +- SubqueryAlias `movies_finances`\n         +- Relation[imdb_id#800,title#801,language#802,revenue#803L,budget#804L,region#805,start_year#806] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:110)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:107)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:126)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:85)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-89ef9d65a4cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0mstart_year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mORDER\u001b[0m \u001b[0mBY\u001b[0m \u001b[0mrevenue\u001b[0m \u001b[0mDESC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m ''')\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \"\"\"\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`imdb_title_id`' given input columns: [movies_finances.title, movies_finances.language, movies_finances.start_year, movies_finances.region, movies_finances.revenue, movies_finances.imdb_id, movies_finances.budget]; line 14 pos 6;\\n'Sort ['revenue DESC NULLS LAST], true\\n+- 'Aggregate ['imdb_title_id, revenue#803L, budget#804L, region#805, language#802, start_year#806], ['imdb_title_id, collect_list(title#801, 0, 0) AS title#814, revenue#803L, budget#804L, region#805, language#802, start_year#806]\\n   +- Filter (isnotnull(revenue#803L) && (isnull(language#802) || (language#802 = en)))\\n      +- SubqueryAlias `movies_finances`\\n         +- Relation[imdb_id#800,title#801,language#802,revenue#803L,budget#804L,region#805,start_year#806] parquet\\n\""
     ]
    }
   ],
   "source": [
    "top_grossing_american_english_movies = spark.sql('''\n",
    "    SELECT \n",
    "      imdb_title_id, \n",
    "      collect_list(title) as title, \n",
    "      revenue, \n",
    "      budget, \n",
    "      region, \n",
    "      language, \n",
    "      start_year \n",
    "    FROM \n",
    "      movies_finances \n",
    "    WHERE revenue IS NOT NULL AND (language IS NULL OR language = 'en') \n",
    "    GROUP BY \n",
    "      imdb_title_id, \n",
    "      revenue, \n",
    "      budget, \n",
    "      region, \n",
    "      language, \n",
    "      start_year \n",
    "    ORDER BY revenue DESC\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_grossing_american_english_movies.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
